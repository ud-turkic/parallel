{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install conllu\n",
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install seaborn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYSCbxILR-WI",
        "outputId": "9dd838df-cf2e-4524-ab5b-9e10007e6b42"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting conllu\n",
            "  Downloading conllu-6.0.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading conllu-6.0.0-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-6.0.0\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import conllu\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple, Counter, Optional\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "\n",
        "class UzbekUDAnalyzer:\n",
        "    \"\"\"Analyzer for Universal Dependencies treebank.\"\"\"\n",
        "\n",
        "    def __init__(self, file_path: str):\n",
        "        \"\"\"Initialize with path to a CoNLL-U file.\"\"\"\n",
        "        self.file_path = file_path\n",
        "        self.sentences = self._load_data()\n",
        "        self.pos_counts = self._count_pos_tags()\n",
        "        self.dependency_counts = self._count_dependency_relations()\n",
        "        self.pos_bigrams = self._count_pos_bigrams()\n",
        "        self.pos_trigrams = self._count_pos_trigrams()\n",
        "\n",
        "    def _load_data(self) -> List:\n",
        "        \"\"\"Load the CoNLL-U file into a list of sentences.\"\"\"\n",
        "        with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return list(conllu.parse(f.read()))\n",
        "\n",
        "    def _count_pos_tags(self) -> Counter:\n",
        "        \"\"\"Count the occurrences of each POS tag.\"\"\"\n",
        "        pos_counter = Counter()\n",
        "        for sentence in self.sentences:\n",
        "            for token in sentence:\n",
        "                if token[\"upos\"]:  # Ensure there's a POS tag\n",
        "                    pos_counter[token[\"upos\"]] += 1\n",
        "        return pos_counter\n",
        "\n",
        "    def _count_dependency_relations(self) -> Counter:\n",
        "        \"\"\"Count the occurrences of each dependency relation.\"\"\"\n",
        "        deprel_counter = Counter()\n",
        "        for sentence in self.sentences:\n",
        "            for token in sentence:\n",
        "                if token[\"deprel\"]:  # Ensure there's a dependency relation\n",
        "                    deprel_counter[token[\"deprel\"]] += 1\n",
        "        return deprel_counter\n",
        "\n",
        "    def _count_pos_bigrams(self) -> Counter:\n",
        "        \"\"\"Count POS tag bigrams.\"\"\"\n",
        "        bigram_counter = Counter()\n",
        "        for sentence in self.sentences:\n",
        "            pos_tags = [token[\"upos\"] for token in sentence if token[\"upos\"]]\n",
        "            for i in range(len(pos_tags) - 1):\n",
        "                bigram = (pos_tags[i], pos_tags[i + 1])\n",
        "                bigram_counter[bigram] += 1\n",
        "        return bigram_counter\n",
        "\n",
        "    def _count_pos_trigrams(self) -> Counter:\n",
        "        \"\"\"Count POS tag trigrams.\"\"\"\n",
        "        trigram_counter = Counter()\n",
        "        for sentence in self.sentences:\n",
        "            pos_tags = [token[\"upos\"] for token in sentence if token[\"upos\"]]\n",
        "            for i in range(len(pos_tags) - 2):\n",
        "                trigram = (pos_tags[i], pos_tags[i + 1], pos_tags[i + 2])\n",
        "                trigram_counter[trigram] += 1\n",
        "        return trigram_counter\n",
        "\n",
        "    def get_token_count(self) -> int:\n",
        "        \"\"\"Return the total number of tokens in the treebank.\"\"\"\n",
        "        return sum(len(sentence) for sentence in self.sentences)\n",
        "\n",
        "    def get_sentence_count(self) -> int:\n",
        "        \"\"\"Return the total number of sentences in the treebank.\"\"\"\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def get_avg_sentence_length(self) -> float:\n",
        "        \"\"\"Calculate the average sentence length in tokens.\"\"\"\n",
        "        return self.get_token_count() / self.get_sentence_count()\n",
        "\n",
        "    def print_basic_stats(self):\n",
        "        \"\"\"Print basic statistics about the treebank.\"\"\"\n",
        "        print(f\"Total sentences: {self.get_sentence_count()}\")\n",
        "        print(f\"Total tokens: {self.get_token_count()}\")\n",
        "        print(f\"Average sentence length: {self.get_avg_sentence_length():.2f} tokens\")\n",
        "        print(f\"Number of unique POS tags: {len(self.pos_counts)}\")\n",
        "        print(f\"Number of unique dependency relations: {len(self.dependency_counts)}\")\n",
        "\n",
        "    def print_pos_distribution(self, top_n: int = 10):\n",
        "        \"\"\"Print the distribution of POS tags.\"\"\"\n",
        "        total = sum(self.pos_counts.values())\n",
        "        print(f\"\\nPOS Tag Distribution (top {top_n}):\")\n",
        "        print(\"=\" * 40)\n",
        "        print(f\"{'POS Tag':<10} {'Count':<10} {'Percentage':<10}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        for pos, count in self.pos_counts.most_common(top_n):\n",
        "            percentage = (count / total) * 100\n",
        "            print(f\"{pos:<10} {count:<10} {percentage:.2f}%\")\n",
        "\n",
        "    def print_dependency_distribution(self, top_n: int = 10):\n",
        "        \"\"\"Print the distribution of dependency relations.\"\"\"\n",
        "        total = sum(self.dependency_counts.values())\n",
        "        print(f\"\\nDependency Relation Distribution (top {top_n}):\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"{'Dependency':<15} {'Count':<10} {'Percentage':<10}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for dep, count in self.dependency_counts.most_common(top_n):\n",
        "            percentage = (count / total) * 100\n",
        "            print(f\"{dep:<15} {count:<10} {percentage:.2f}%\")\n",
        "\n",
        "    def print_common_pos_bigrams(self, top_n: int = 10):\n",
        "        \"\"\"Print the most common POS bigrams.\"\"\"\n",
        "        total = sum(self.pos_bigrams.values())\n",
        "        print(f\"\\nCommon POS Bigrams (top {top_n}):\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"{'Bigram':<25} {'Count':<10} {'Percentage':<10}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        for bigram, count in self.pos_bigrams.most_common(top_n):\n",
        "            percentage = (count / total) * 100\n",
        "            print(f\"{' → '.join(bigram):<25} {count:<10} {percentage:.2f}%\")\n",
        "\n",
        "    def print_common_pos_trigrams(self, top_n: int = 10):\n",
        "        \"\"\"Print the most common POS trigrams.\"\"\"\n",
        "        total = sum(self.pos_trigrams.values())\n",
        "        print(f\"\\nCommon POS Trigrams (top {top_n}):\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"{'Trigram':<35} {'Count':<10} {'Percentage':<10}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        for trigram, count in self.pos_trigrams.most_common(top_n):\n",
        "            percentage = (count / total) * 100\n",
        "            print(f\"{' → '.join(trigram):<35} {count:<10} {percentage:.2f}%\")\n",
        "\n",
        "    def analyze_verb_position(self):\n",
        "        \"\"\"Analyze the position of verbs in sentences.\"\"\"\n",
        "        positions = []\n",
        "        for sentence in self.sentences:\n",
        "            sentence_length = len(sentence)\n",
        "            for token_idx, token in enumerate(sentence):\n",
        "                if token[\"upos\"] == \"VERB\":\n",
        "                    # Calculate relative position (0-1 scale)\n",
        "                    relative_pos = token_idx / max(1, sentence_length - 1)\n",
        "                    positions.append(relative_pos)\n",
        "\n",
        "        # Calculate statistics\n",
        "        if positions:\n",
        "            avg_pos = sum(positions) / len(positions)\n",
        "            final_pos_count = sum(1 for p in positions if p > 0.9)\n",
        "            final_pos_pct = (final_pos_count / len(positions)) * 100\n",
        "\n",
        "            print(\"\\nVerb Position Analysis:\")\n",
        "            print(\"=\" * 40)\n",
        "            print(f\"Total verbs analyzed: {len(positions)}\")\n",
        "            print(f\"Average relative position: {avg_pos:.2f} (0=start, 1=end)\")\n",
        "            print(f\"Verbs in final position (>90%): {final_pos_count} ({final_pos_pct:.2f}%)\")\n",
        "\n",
        "            # Create histogram\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.hist(positions, bins=10, alpha=0.7, color='blue')\n",
        "            plt.title('Distribution of Verb Positions in Sentences')\n",
        "            plt.xlabel('Relative Position (0=start, 1=end)')\n",
        "            plt.ylabel('Frequency')\n",
        "            plt.grid(alpha=0.3)\n",
        "            plt.savefig('verb_positions.png')\n",
        "            plt.close()\n",
        "            print(\"Generated 'verb_positions.png' - histogram of verb positions\")\n",
        "        else:\n",
        "            print(\"No verbs found in the treebank\")\n",
        "\n",
        "    def analyze_det_adj_noun_order(self):\n",
        "        \"\"\"Analyze determiner-adjective-noun order in the treebank.\"\"\"\n",
        "        det_adj_noun = 0\n",
        "        adj_det_noun = 0\n",
        "\n",
        "        for sentence in self.sentences:\n",
        "            pos_tags = [token[\"upos\"] for token in sentence]\n",
        "\n",
        "            for i in range(len(pos_tags) - 1):\n",
        "                if pos_tags[i] == \"DET\" and pos_tags[i+1] == \"ADJ\" and pos_tags[i+2] == \"NOUN\":\n",
        "                    det_adj_noun += 1\n",
        "                elif pos_tags[i] == \"ADJ\" and pos_tags[i+1] == \"DET\" and pos_tags[i+2] == \"NOUN\":\n",
        "                    adj_det_noun += 1\n",
        "\n",
        "        total = det_adj_noun + adj_det_noun\n",
        "        if total > 0:\n",
        "            print(\"\\nAdjective-Noun Order Analysis:\")\n",
        "            print(\"=\" * 40)\n",
        "            print(f\"Determiner before Adjective: {det_adj_noun} ({det_adj_noun/total*100:.2f}%)\")\n",
        "            print(f\"Adjective before Determiner: {adj_det_noun} ({adj_det_noun/total*100:.2f}%)\")\n",
        "        else:\n",
        "            print(\"\\nNo determiner-adjective-noun pairs found in the treebank\")\n",
        "\n",
        "    def analyze_adposition_usage(self):\n",
        "        \"\"\"Analyze whether the language uses prepositions or postpositions.\"\"\"\n",
        "        prepositions = 0\n",
        "        postpositions = 0\n",
        "\n",
        "        for sentence in self.sentences:\n",
        "            for token_idx, token in enumerate(sentence):\n",
        "                if token[\"upos\"] == \"ADP\":\n",
        "                    head_idx = token[\"head\"] - 1  # CoNLL-U uses 1-based indexing\n",
        "\n",
        "                    # Skip if head is out of bounds\n",
        "                    if head_idx < 0 or head_idx >= len(sentence):\n",
        "                        continue\n",
        "\n",
        "                    if token_idx < head_idx:  # ADP comes before its head\n",
        "                        prepositions += 1\n",
        "                    else:  # ADP comes after its head\n",
        "                        postpositions += 1\n",
        "\n",
        "        total = prepositions + postpositions\n",
        "        if total > 0:\n",
        "            print(\"\\nAdposition Usage Analysis:\")\n",
        "            print(\"=\" * 40)\n",
        "            print(f\"Prepositions: {prepositions} ({prepositions/total*100:.2f}%)\")\n",
        "            print(f\"Postpositions: {postpositions} ({postpositions/total*100:.2f}%)\")\n",
        "        else:\n",
        "            print(\"\\nNo adpositions found in the treebank\")\n",
        "\n",
        "    def analyze_subject_verb_object_order(self):\n",
        "        \"\"\"Analyze the order of subject, verb, and object.\"\"\"\n",
        "        # Dictionary to store counts of each order\n",
        "        orders = {\n",
        "            \"SOV\": 0, \"SVO\": 0, \"VSO\": 0,\n",
        "            \"VOS\": 0, \"OVS\": 0, \"OSV\": 0\n",
        "        }\n",
        "\n",
        "        # Initialize counts of valid clauses with clear SVO elements\n",
        "        valid_clauses = 0\n",
        "\n",
        "        for sentence in self.sentences:\n",
        "            # Find subject, verb, and object in the sentence\n",
        "            s_pos, v_pos, o_pos = None, None, None\n",
        "\n",
        "            for token in sentence:\n",
        "                if token[\"deprel\"] == \"nsubj\" and token[\"head\"] > 0:\n",
        "                    # The subject's position\n",
        "                    s_pos = token[\"id\"] - 1\n",
        "\n",
        "                    # The verb position (head of the subject)\n",
        "                    head_idx = token[\"head\"] - 1\n",
        "                    if head_idx < len(sentence) and sentence[head_idx][\"upos\"] in [\"VERB\", \"AUX\"]:\n",
        "                        v_pos = head_idx\n",
        "\n",
        "            # Find direct object connected to the verb\n",
        "            if v_pos is not None:\n",
        "                verb_id = v_pos + 1  # Convert to 1-based index\n",
        "                for token in sentence:\n",
        "                    if token[\"deprel\"] == \"obj\" and token[\"head\"] == verb_id:\n",
        "                        o_pos = token[\"id\"] - 1\n",
        "\n",
        "            # If we have all three components, determine order\n",
        "            if s_pos is not None and v_pos is not None and o_pos is not None:\n",
        "                valid_clauses += 1\n",
        "                positions = {s_pos: \"S\", v_pos: \"V\", o_pos: \"O\"}\n",
        "                order = \"\".join(positions[pos] for pos in sorted([s_pos, v_pos, o_pos]))\n",
        "                orders[order] += 1\n",
        "\n",
        "        if valid_clauses > 0:\n",
        "            print(\"\\nSubject-Verb-Object Order Analysis:\")\n",
        "            print(\"=\" * 40)\n",
        "            print(f\"Valid clauses analyzed: {valid_clauses}\")\n",
        "\n",
        "            # Sort orders by frequency (descending)\n",
        "            sorted_orders = sorted(orders.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            for order, count in sorted_orders:\n",
        "                if count > 0:\n",
        "                    print(f\"{order}: {count} ({count/valid_clauses*100:.2f}%)\")\n",
        "\n",
        "            # Create bar chart\n",
        "            orders_list = [order for order, count in sorted_orders if count > 0]\n",
        "            counts = [count for order, count in sorted_orders if count > 0]\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            bars = plt.bar(orders_list, counts, color='skyblue')\n",
        "            plt.title('Subject-Verb-Object Order Distribution')\n",
        "            plt.xlabel('Word Order')\n",
        "            plt.ylabel('Frequency')\n",
        "\n",
        "            # Add percentage labels on top of bars\n",
        "            for bar, count in zip(bars, counts):\n",
        "                height = bar.get_height()\n",
        "                plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "                        f'{count/valid_clauses*100:.1f}%',\n",
        "                        ha='center', va='bottom')\n",
        "\n",
        "            plt.savefig('svo_order.png')\n",
        "            plt.close()\n",
        "            print(\"Generated 'svo_order.png' - bar chart of word order distribution\")\n",
        "        else:\n",
        "            print(\"\\nNo valid subject-verb-object clauses found in the treebank\")\n",
        "\n",
        "    def visualize_pos_distribution(self):\n",
        "        \"\"\"Create a bar chart of POS tag distribution.\"\"\"\n",
        "        top_pos = dict(self.pos_counts.most_common(10))\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        bars = plt.bar(top_pos.keys(), top_pos.values(), color='teal')\n",
        "        plt.title('Distribution of POS Tags')\n",
        "        plt.xlabel('POS Tag')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        # Add count labels on top of bars\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "                    f'{height}', ha='center', va='bottom')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('pos_distribution.png')\n",
        "        plt.close()\n",
        "        print(\"\\nGenerated 'pos_distribution.png' - bar chart of POS tag distribution\")\n",
        "\n",
        "    def analyze_pos_transitions(self):\n",
        "        \"\"\"Analyze and visualize common transitions between POS tags.\"\"\"\n",
        "        # Create a transition matrix\n",
        "        unique_pos = list(self.pos_counts.keys())\n",
        "        num_pos = len(unique_pos)\n",
        "        pos_to_idx = {pos: idx for idx, pos in enumerate(unique_pos)}\n",
        "\n",
        "        # Initialize transition matrix\n",
        "        transition_matrix = np.zeros((num_pos, num_pos))\n",
        "\n",
        "        # Fill the transition matrix\n",
        "        for bigram, count in self.pos_bigrams.items():\n",
        "            if len(bigram) == 2:  # Ensure it's a valid bigram\n",
        "                i, j = pos_to_idx[bigram[0]], pos_to_idx[bigram[1]]\n",
        "                transition_matrix[i, j] = count\n",
        "\n",
        "        # Convert to probability matrix\n",
        "        row_sums = transition_matrix.sum(axis=1, keepdims=True)\n",
        "        prob_matrix = np.zeros_like(transition_matrix)\n",
        "        np.divide(transition_matrix, row_sums, out=prob_matrix, where=row_sums!=0)\n",
        "\n",
        "        # Plot the heatmap\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(prob_matrix, annot=True, fmt='.2f', cmap='viridis',\n",
        "                   xticklabels=unique_pos, yticklabels=unique_pos)\n",
        "        plt.title('POS Tag Transition Probabilities')\n",
        "        plt.xlabel('To POS')\n",
        "        plt.ylabel('From POS')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('pos_transitions.png')\n",
        "        plt.close()\n",
        "        print(\"\\nGenerated 'pos_transitions.png' - heatmap of POS tag transitions\")\n",
        "\n",
        "        # Return the most common transition for each POS tag\n",
        "        most_common_next = {}\n",
        "        for i, pos in enumerate(unique_pos):\n",
        "            if row_sums[i][0] > 0:  # Check if the POS tag has any transitions\n",
        "                next_pos_idx = np.argmax(prob_matrix[i])\n",
        "                prob = prob_matrix[i][next_pos_idx]\n",
        "                most_common_next[pos] = (unique_pos[next_pos_idx], prob)\n",
        "\n",
        "        print(\"\\nMost Common POS Tag Transitions:\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"{'From POS':<10} {'To POS':<10} {'Probability':<10}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for pos, (next_pos, prob) in sorted(most_common_next.items()):\n",
        "            print(f\"{pos:<10} {next_pos:<10} {prob:.4f}\")\n",
        "\n",
        "# Example usage\n",
        "def analyze_treebank(file_path):\n",
        "    analyzer = UzbekUDAnalyzer(file_path)\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "    print(\"UD TREEBANK ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Basic statistics\n",
        "    analyzer.print_basic_stats()\n",
        "\n",
        "    # POS distribution\n",
        "    analyzer.print_pos_distribution()\n",
        "    analyzer.visualize_pos_distribution()\n",
        "\n",
        "    # Dependency distribution\n",
        "    analyzer.print_dependency_distribution()\n",
        "\n",
        "    # Common POS sequences\n",
        "    analyzer.print_common_pos_bigrams(15)\n",
        "    analyzer.print_common_pos_trigrams(10)\n",
        "\n",
        "    # Word order analysis\n",
        "    analyzer.analyze_verb_position()\n",
        "    analyzer.analyze_det_adj_noun_order()\n",
        "    analyzer.analyze_adposition_usage()\n",
        "    analyzer.analyze_subject_verb_object_order()\n",
        "\n",
        "    # POS transitions\n",
        "    analyzer.analyze_pos_transitions()\n",
        "\n",
        "# To use the script, call the function with the path to your CoNLL-U file:\n",
        "# analyze_uzbek_treebank(\"path/to/your/uzbek_treebank.conllu\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with your actual file path\n",
        "    conllu_file = input(\"Enter the path to your UD treebank file (CoNLL-U format): \")\n",
        "    analyze_treebank(conllu_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9DlauqZZMDd",
        "outputId": "f3b68fc1-348e-4b47-deca-a3362048446f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the path to your UD treebank file (CoNLL-U format): /content/tr_tuecl-ud-test.fa.conllu\n",
            "==================================================\n",
            "UZBEK UD TREEBANK ANALYSIS\n",
            "==================================================\n",
            "Total sentences: 148\n",
            "Total tokens: 917\n",
            "Average sentence length: 6.20 tokens\n",
            "Number of unique POS tags: 15\n",
            "Number of unique dependency relations: 39\n",
            "\n",
            "POS Tag Distribution (top 10):\n",
            "========================================\n",
            "POS Tag    Count      Percentage\n",
            "----------------------------------------\n",
            "NOUN       220        23.99%\n",
            "VERB       175        19.08%\n",
            "PUNCT      167        18.21%\n",
            "PROPN      92         10.03%\n",
            "AUX        76         8.29%\n",
            "ADJ        44         4.80%\n",
            "ADV        42         4.58%\n",
            "PRON       40         4.36%\n",
            "_          19         2.07%\n",
            "CCONJ      14         1.53%\n",
            "\n",
            "Generated 'pos_distribution.png' - bar chart of POS tag distribution\n",
            "\n",
            "Dependency Relation Distribution (top 10):\n",
            "==================================================\n",
            "Dependency      Count      Percentage\n",
            "--------------------------------------------------\n",
            "punct           167        18.21%\n",
            "root            148        16.14%\n",
            "nsubj           122        13.30%\n",
            "obl             71         7.74%\n",
            "obj             61         6.65%\n",
            "advmod          34         3.71%\n",
            "aux             29         3.16%\n",
            "aux:q           26         2.84%\n",
            "conj            22         2.40%\n",
            "ccomp           19         2.07%\n",
            "\n",
            "Common POS Bigrams (top 15):\n",
            "============================================================\n",
            "Bigram                    Count      Percentage\n",
            "------------------------------------------------------------\n",
            "VERB → PUNCT              92         11.96%\n",
            "NOUN → VERB               78         10.14%\n",
            "NOUN → NOUN               55         7.15%\n",
            "AUX → PUNCT               50         6.50%\n",
            "PROPN → NOUN              48         6.24%\n",
            "VERB → AUX                29         3.77%\n",
            "VERB → VERB               27         3.51%\n",
            "NOUN → AUX                19         2.47%\n",
            "ADJ → PUNCT               15         1.95%\n",
            "ADV → VERB                15         1.95%\n",
            "PROPN → VERB              14         1.82%\n",
            "NOUN → PROPN              14         1.82%\n",
            "VERB → NOUN               14         1.82%\n",
            "PRON → VERB               13         1.69%\n",
            "NOUN → ADJ                13         1.69%\n",
            "\n",
            "Common POS Trigrams (top 10):\n",
            "======================================================================\n",
            "Trigram                             Count      Percentage\n",
            "----------------------------------------------------------------------\n",
            "NOUN → VERB → PUNCT                 40         6.44%\n",
            "VERB → AUX → PUNCT                  24         3.86%\n",
            "NOUN → NOUN → VERB                  23         3.70%\n",
            "VERB → VERB → PUNCT                 16         2.58%\n",
            "PROPN → NOUN → VERB                 16         2.58%\n",
            "NOUN → NOUN → NOUN                  13         2.09%\n",
            "NOUN → VERB → VERB                  12         1.93%\n",
            "NOUN → AUX → PUNCT                  12         1.93%\n",
            "ADV → VERB → PUNCT                  11         1.77%\n",
            "NOUN → VERB → AUX                   11         1.77%\n",
            "\n",
            "Verb Position Analysis:\n",
            "========================================\n",
            "Total verbs analyzed: 175\n",
            "Average relative position: 0.57 (0=start, 1=end)\n",
            "Verbs in final position (>90%): 6 (3.43%)\n",
            "Generated 'verb_positions.png' - histogram of verb positions\n",
            "\n",
            "Adjective-Noun Order Analysis:\n",
            "========================================\n",
            "Determiner before Adjective: 0 (0.00%)\n",
            "Adjective before Determiner: 3 (100.00%)\n",
            "\n",
            "Adposition Usage Analysis:\n",
            "========================================\n",
            "Prepositions: 0 (0.00%)\n",
            "Postpositions: 3 (100.00%)\n",
            "\n",
            "Subject-Verb-Object Order Analysis:\n",
            "========================================\n",
            "Valid clauses analyzed: 29\n",
            "SOV: 29 (100.00%)\n",
            "Generated 'svo_order.png' - bar chart of word order distribution\n",
            "\n",
            "Generated 'pos_transitions.png' - heatmap of POS tag transitions\n",
            "\n",
            "Most Common POS Tag Transitions:\n",
            "==================================================\n",
            "From POS   To POS     Probability\n",
            "--------------------------------------------------\n",
            "ADJ        PUNCT      0.3409\n",
            "ADP        VERB       0.3333\n",
            "ADV        VERB       0.3571\n",
            "AUX        PUNCT      0.6579\n",
            "CCONJ      NOUN       0.3571\n",
            "DET        NOUN       0.9286\n",
            "INTJ       _          1.0000\n",
            "NOUN       VERB       0.3545\n",
            "NUM        NOUN       0.6667\n",
            "PRON       VERB       0.3250\n",
            "PROPN      NOUN       0.5217\n",
            "PUNCT      NOUN       0.3158\n",
            "SCONJ      NOUN       0.2500\n",
            "VERB       PUNCT      0.5257\n",
            "_          NOUN       0.3684\n"
          ]
        }
      ]
    }
  ]
}